{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a SPAM Filter using Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Enron Email corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Enron Email Corpus is one of the biggest email data sources in the world. It has 600,000 emails spread over 2.5 GB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficult part of the project is reading almost half-a-million files. All the emails are in the MIME format - Multipurpose Internet Mail Extensions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is obtained from : http://www2.aueb.gr/users/ion/data/enron-spam/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset comprises of 6 different folders, which contains different emails labelled as SPAM and HAM. Let us have a look at the directories and files in folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OS module in Python provides a way of using operating system dependent\n",
    "functionality. \n",
    "\n",
    "The functions that the OS module provides allows you to interface with the\n",
    "underlying operating system that Python is running on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron ['enron1', 'enron6', 'enron5', 'enron2', 'enron3', 'enron4'] 1\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron1 ['spam', 'ham'] 2\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron1/spam [] 1500\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron1/ham [] 3672\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron6 ['spam', 'ham'] 1\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron6/spam [] 4500\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron6/ham [] 1500\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron5 ['spam', 'ham'] 1\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron5/spam [] 3675\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron5/ham [] 1500\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron2 ['spam', 'ham'] 1\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron2/spam [] 1496\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron2/ham [] 4361\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron3 ['spam', 'ham'] 1\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron3/spam [] 1500\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron3/ham [] 4012\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron4 ['spam', 'ham'] 1\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron4/spam [] 4500\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron4/ham [] 1500\n"
     ]
    }
   ],
   "source": [
    "for directories, subdirs, files in os.walk(\"/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron\"):\n",
    "    print(directories, subdirs, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, instead of looping through all the available folders, we will loop over files only in the HAM and SPAM folders.\n",
    "This is to make sure that we read only the files from SPAM and HAM folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron1/spam [] 1500\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron1/ham [] 3672\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron6/spam [] 4500\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron6/ham [] 1500\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron5/spam [] 3675\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron5/ham [] 1500\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron2/spam [] 1496\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron2/ham [] 4361\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron3/spam [] 1500\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron3/ham [] 4012\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron4/spam [] 4500\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron4/ham [] 1500\n"
     ]
    }
   ],
   "source": [
    "for directories, subdirs, files in os.walk(\"/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron\"):\n",
    "    if (os.path.split(directories)[1]=='ham'):\n",
    "        print(directories, subdirs, len(files))\n",
    "        \n",
    "    if (os.path.split(directories)[1]=='spam'):\n",
    "        print(directories, subdirs, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us read the spam and ham emails by appending each of them to their respective lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ham_list = []\n",
    "spam_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for directories,subdirs,files in os.walk(\"/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron\"):\n",
    "    if(os.path.split(directories)[1]=='ham'):\n",
    "        for filename in files:\n",
    "            with open(os.path.join(directories,filename),encoding='latin-1') as f:\n",
    "                data = f.read()\n",
    "                ham_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for directories,subdirs,files in os.walk(\"/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron\"):\n",
    "    if(os.path.split(directories)[1]=='spam'):\n",
    "        for filename in files:\n",
    "            with open(os.path.join(directories,filename),encoding='latin-1') as f:\n",
    "                data = f.read()\n",
    "                spam_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: what up , , your cam babe\n",
      "what are you looking for ?\n",
      "if your looking for a companion for friendship , love , a date , or just good ole '\n",
      "fashioned * * * * * * , then try our brand new site ; it was developed and created\n",
      "to help anyone find what they ' re looking for . a quick bio form and you ' re\n",
      "on the road to satisfaction in every sense of the word . . . . no matter what\n",
      "that may be !\n",
      "try it out and youll be amazed .\n",
      "have a terrific time this evening\n",
      "copy and pa ste the add . ress you see on the line below into your browser to come to the site .\n",
      "http : / / www . meganbang . biz / bld / acc /\n",
      "no more plz\n",
      "http : / / www . naturalgolden . com / retract /\n",
      "counterattack aitken step preemptive shoehorn scaup . electrocardiograph movie honeycomb . monster war brandywine pietism byrne catatonia . encomia lookup intervenor skeleton turn catfish .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(spam_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task would be preparing the data for Naive Bayes classifier. The Naive Bayes is a fairly simple machine learning algorithm, that works mainly with probabilities.\n",
    "\n",
    "It consists of multiple steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step would be defining a function for creating word features, which prepares each word in the required input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is how the Naive Bayes classifier expects the input\n",
    "def create_word_features(words):\n",
    "    useful_words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    my_dict = dict([(word, True) for word in useful_words])\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case, instead of reading data as strings and appending it to a list. We tokenize each string into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron1\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron1/spam\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron1/ham\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron6\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron6/spam\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron6/ham\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron5\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron5/spam\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron5/ham\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron2\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron2/spam\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron2/ham\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron3\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron3/spam\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron3/ham\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron4\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron4/spam\n",
      "/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron/enron4/ham\n"
     ]
    }
   ],
   "source": [
    "ham_data=[]\n",
    "spam_data=[]\n",
    "for directories,subdirs,files in os.walk(\"/Users/saiyesaswymylavarapu/Documents/Documents/PythonBootCamp/Spam Filter/Enron\"):\n",
    "    print(directories)\n",
    "    if(os.path.split(directories)[1]=='ham'):\n",
    "        for filename in files:\n",
    "            with open(os.path.join(directories,filename),encoding='latin-1') as f:\n",
    "                data = f.read()\n",
    "                #Tokenizing sentences into words\n",
    "                words = word_tokenize(data)\n",
    "                #Creating word features for all the words\n",
    "                ham_data.append((create_word_features(words),\"ham\"))\n",
    "    \n",
    "    if(os.path.split(directories)[1]=='spam'):\n",
    "        for filename in files:\n",
    "            with open(os.path.join(directories,filename),encoding='latin-1') as f:\n",
    "                data = f.read()\n",
    "                #Tokenizing sentences into words\n",
    "                words = word_tokenize(data)\n",
    "                #Creating word features for all the words\n",
    "                spam_data.append((create_word_features(words),\"spam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'!': True,\n",
       "  '&': True,\n",
       "  \"'\": True,\n",
       "  '(': True,\n",
       "  ')': True,\n",
       "  ',': True,\n",
       "  '-': True,\n",
       "  '.': True,\n",
       "  '/': True,\n",
       "  '04': True,\n",
       "  '05': True,\n",
       "  '07': True,\n",
       "  '08': True,\n",
       "  '10': True,\n",
       "  '11': True,\n",
       "  '2000': True,\n",
       "  '56': True,\n",
       "  '7': True,\n",
       "  '99': True,\n",
       "  ':': True,\n",
       "  '?': True,\n",
       "  '@': True,\n",
       "  'Subject': True,\n",
       "  'accomplish': True,\n",
       "  'activity': True,\n",
       "  'advance': True,\n",
       "  'agreement': True,\n",
       "  'allow': True,\n",
       "  'altrade': True,\n",
       "  'anything': True,\n",
       "  'based': True,\n",
       "  'begin': True,\n",
       "  'behalf': True,\n",
       "  'brazoria': True,\n",
       "  'brenda': True,\n",
       "  'buy': True,\n",
       "  'buys': True,\n",
       "  'c': True,\n",
       "  'carbide': True,\n",
       "  'cc': True,\n",
       "  'central': True,\n",
       "  'ces': True,\n",
       "  'check': True,\n",
       "  'chemicals': True,\n",
       "  'cheryl': True,\n",
       "  'christi': True,\n",
       "  'city': True,\n",
       "  'clynes': True,\n",
       "  'come': True,\n",
       "  'company': True,\n",
       "  'contracts': True,\n",
       "  'contractual': True,\n",
       "  'corporation': True,\n",
       "  'corpus': True,\n",
       "  'could': True,\n",
       "  'counterparties': True,\n",
       "  'customer': True,\n",
       "  'customers': True,\n",
       "  'daren': True,\n",
       "  'deals': True,\n",
       "  'desk': True,\n",
       "  'distribution': True,\n",
       "  'done': True,\n",
       "  'draft': True,\n",
       "  'dudley': True,\n",
       "  'duke': True,\n",
       "  'ect': True,\n",
       "  'eliminate': True,\n",
       "  'else': True,\n",
       "  'ena': True,\n",
       "  'energy': True,\n",
       "  'entered': True,\n",
       "  'entex': True,\n",
       "  'equistar': True,\n",
       "  'everything': True,\n",
       "  'exactly': True,\n",
       "  'familiar': True,\n",
       "  'farmer': True,\n",
       "  'field': True,\n",
       "  'following': True,\n",
       "  'forwarded': True,\n",
       "  'fuel': True,\n",
       "  'gas': True,\n",
       "  'gathering': True,\n",
       "  'get': True,\n",
       "  'gets': True,\n",
       "  'give': True,\n",
       "  'gulf': True,\n",
       "  'h': True,\n",
       "  'happen': True,\n",
       "  'help': True,\n",
       "  'herod': True,\n",
       "  'hl': True,\n",
       "  'hou': True,\n",
       "  'hpl': True,\n",
       "  'hplc': True,\n",
       "  'ideas': True,\n",
       "  'illinois': True,\n",
       "  'inc': True,\n",
       "  'j': True,\n",
       "  'king': True,\n",
       "  'know': True,\n",
       "  'l': True,\n",
       "  'light': True,\n",
       "  'like': True,\n",
       "  'long': True,\n",
       "  'looking': True,\n",
       "  'lp': True,\n",
       "  'make': True,\n",
       "  'marketing': True,\n",
       "  'markets': True,\n",
       "  'mary': True,\n",
       "  'mentioned': True,\n",
       "  'mills': True,\n",
       "  'missing': True,\n",
       "  'move': True,\n",
       "  'need': True,\n",
       "  'needs': True,\n",
       "  'new': True,\n",
       "  'nowadays': True,\n",
       "  'offer': True,\n",
       "  'one': True,\n",
       "  'ones': True,\n",
       "  'order': True,\n",
       "  'p': True,\n",
       "  'panther': True,\n",
       "  'pat': True,\n",
       "  'pipe': True,\n",
       "  'pipeline': True,\n",
       "  'pm': True,\n",
       "  'power': True,\n",
       "  'praxair': True,\n",
       "  'project': True,\n",
       "  'ran': True,\n",
       "  'real': True,\n",
       "  'receptive': True,\n",
       "  'reliant': True,\n",
       "  'report': True,\n",
       "  'response': True,\n",
       "  'right': True,\n",
       "  'run': True,\n",
       "  'sales': True,\n",
       "  'scott': True,\n",
       "  'sell': True,\n",
       "  'selling': True,\n",
       "  'sells': True,\n",
       "  'sent': True,\n",
       "  'services': True,\n",
       "  'show': True,\n",
       "  'showing': True,\n",
       "  'since': True,\n",
       "  'sitara': True,\n",
       "  'smith': True,\n",
       "  'someone': True,\n",
       "  'something': True,\n",
       "  'southern': True,\n",
       "  'spot': True,\n",
       "  'status': True,\n",
       "  'storage': True,\n",
       "  'subject': True,\n",
       "  'suggested': True,\n",
       "  'suggestions': True,\n",
       "  'sure': True,\n",
       "  'tell': True,\n",
       "  'term': True,\n",
       "  'texas': True,\n",
       "  'thanks': True,\n",
       "  'track': True,\n",
       "  'transaction': True,\n",
       "  'transmission': True,\n",
       "  'transport': True,\n",
       "  'transports': True,\n",
       "  'trying': True,\n",
       "  'txu': True,\n",
       "  'union': True,\n",
       "  'unit': True,\n",
       "  'update': True,\n",
       "  'utilities': True,\n",
       "  'volumes': True,\n",
       "  'waiting': True,\n",
       "  'wondering': True,\n",
       "  'working': True,\n",
       "  'would': True},\n",
       " 'ham')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is creating Train and test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ham and Spam data is initially combined and shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "total_data = ham_data + spam_data\n",
    "random.shuffle(total_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and test datasets by splitting into 70% and 30% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23601\n",
      "10115\n"
     ]
    }
   ],
   "source": [
    "train_part = int(len(total_data) * .7)\n",
    "\n",
    "train = total_data[:train_part]\n",
    " \n",
    "test =  total_data[train_part:]\n",
    " \n",
    "print (len(train))\n",
    "print (len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  98.92239248640632\n"
     ]
    }
   ],
   "source": [
    "# Create the Naive Bayes filter\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train)\n",
    " \n",
    "# Find the accuracy, using the test data\n",
    " \n",
    "accuracy = nltk.classify.util.accuracy(classifier, test)\n",
    " \n",
    "print(\"Accuracy is: \", accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the accuracy of our model turned out to be 98.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the most informative features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    meds = True             spam : ham    =    299.0 : 1.0\n",
      "                     xls = True              ham : spam   =    287.4 : 1.0\n",
      "                     713 = True              ham : spam   =    271.1 : 1.0\n",
      "                 stinson = True              ham : spam   =    268.4 : 1.0\n",
      "                crenshaw = True              ham : spam   =    268.4 : 1.0\n",
      "                     ect = True              ham : spam   =    226.2 : 1.0\n",
      "                     eol = True              ham : spam   =    212.9 : 1.0\n",
      "             medications = True             spam : ham    =    209.1 : 1.0\n",
      "              scheduling = True              ham : spam   =    205.6 : 1.0\n",
      "                  louise = True              ham : spam   =    200.5 : 1.0\n",
      "                     hpl = True              ham : spam   =    194.4 : 1.0\n",
      "                 parsing = True              ham : spam   =    192.7 : 1.0\n",
      "                     oem = True             spam : ham    =    173.6 : 1.0\n",
      "                    pill = True             spam : ham    =    166.4 : 1.0\n",
      "                   daren = True              ham : spam   =    163.3 : 1.0\n",
      "                    1933 = True             spam : ham    =    144.4 : 1.0\n",
      "               schedules = True              ham : spam   =    136.9 : 1.0\n",
      "                     sex = True             spam : ham    =    132.4 : 1.0\n",
      "                   penis = True             spam : ham    =    125.1 : 1.0\n",
      "                      um = True             spam : ham    =    124.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that words like medications, pill and sex are most likely to occur in SPAM emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking on some examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have taken some examples of SPAM and HAM emails from the web and tried to determine their class based on my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mail1 = '''It is truly incredible - People from all around the world are using\n",
    "their webcams to get off. Now is your chance to watch Men and women,\n",
    "boys and girls show off just for you. Best of all, it's FREE, LIVE\n",
    "and UN-F*CKING-BELIEVABLE. Either peep in on the sexy activity or\n",
    "participate with your own webcam! You've got to try this out!\n",
    "Open Amateur Webcam Feeds are Active RIGHT NOW!!!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email 1 is : spam\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(mail1)\n",
    "features = create_word_features(words)\n",
    "print(\"Email 1 is :\" ,classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mail2 = '''Dear recipient,\n",
    "Avangar Technologies announces the beginning of a new unprecendented global employment campaign. \n",
    "reviser yeller winers butchery twenties\n",
    "Due to company's exploding growth Avangar is expanding business to the European region.\n",
    "During last employment campaign over 1500 people worldwide took part in Avangar's business\n",
    "and more than half of them are currently employed by the company. And now we are offering you\n",
    "one more opportunity to earn extra money working with Avangar Technologies.\n",
    "druggists blame classy gentry Aladdin\n",
    "\n",
    "We are looking for honest, responsible, hard-working people that can dedicate 2-4 hours of their\n",
    "time per day and earn extra Â£300-500 weekly. All offered positions are currently part-time\n",
    "and give you a chance to work mainly from home.\n",
    "lovelies hockey Malton meager reordered\n",
    "\n",
    "Please visit Avangar's corporate web site (http://www.avangar.com/sta/home/0077.htm) for more details regarding these vacancies.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email 2 is : spam\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(mail2)\n",
    "features = create_word_features(words)\n",
    "print(\"Email 2 is :\" ,classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mail3 = '''Dear Mr. James:\n",
    "\n",
    "I sincerely enjoyed meeting with you yesterday and learning more about the Position at Employer.\n",
    "\n",
    "Our conversation confirmed my interest in becoming part of Employer's staff. I was particularly pleased at the prospect of being able to develop my own article ideas with the head of the bureau, and develop my multi-media skills.\n",
    "\n",
    "I feel confident that my experiences both in the workplace and in the classroom would enable me to fill the job requirements effectively.\n",
    "\n",
    "Please feel free to contact me if I can provide you with any further information.\n",
    "\n",
    "I look forward eagerly to hearing from you, and thank you again for the courtesy you extended to me.\n",
    "\n",
    "Sincerely'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email 3 is : ham\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(mail3)\n",
    "features = create_word_features(words)\n",
    "print(\"Email 3 is :\" ,classifier.classify(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#conda install requests will be needed\n",
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(classifier, open(\"spam_class.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_spam_classifier = pickle.load(open(\"spam_class.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.classify.naivebayes.NaiveBayesClassifier at 0x10cec5ef0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_spam_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will start a flask service. That's in the file 'flask_demo.py'\n",
    "\n",
    "Once it's started, we can use this bit of code to call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
